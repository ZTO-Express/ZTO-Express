<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>中通技术团队</title>
  
  <subtitle>中通技术团队</subtitle>
  <link href="https://zto-express.github.io/atom.xml" rel="self"/>
  
  <link href="https://zto-express.github.io/"/>
  <updated>2020-08-15T03:37:03.132Z</updated>
  <id>https://zto-express.github.io/</id>
  
  <author>
    <name>[object Object]</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>中通Elasticsearch集群运维实践（二）--监控告警</title>
    <link href="https://zto-express.github.io/2020/08/15/es/"/>
    <id>https://zto-express.github.io/2020/08/15/es/</id>
    <published>2020-08-15T02:32:54.000Z</published>
    <updated>2020-08-15T03:37:03.132Z</updated>
    
    <content type="html"><![CDATA[<h4 id="背景介绍："><a href="#背景介绍：" class="headerlink" title="背景介绍："></a>背景介绍：</h4><p>中通在2015年的时候已经开始预研并在生产环境使用Elasticsearch集群，随后在科技中心开始大规模实践。随着业务的快速发展，es集群数量和规模也越来越大，版本的跨度也逐渐拉大，统一管理这些es集群逐渐变成了首要问题，在这种情况下，我们研发了中通ES运维监控平台–ESPaaS，提供了ES集群的自动化部署，统一监控，实时告警和索引管理等一系列运维管理功能，截止2020年7月底，中通生产上运行的es集群数量已经有40+个，节点数量500+个，单个集群的节点数从3个到100多个，单日新增文档数量近600亿，单日数据增量超过100tb，数据总量已经超6PB。</p><p>不同的阶段，关注和解决的问题也不一样，ESPaaS平台的版本迭代主要分为以下几个阶段：</p><img src="time-line.png" style="zoom： 50%；" /><p>本文主要介绍的是中通ESPaaS运维平台在统一监控告警上的实践，主要关注以下内容：</p><ol><li>集群实时监控</li><li>告警输出</li><li>集群诊断</li></ol><h4 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h4><p><code>prometheus</code>作为现在最流行的监控解决方案之一，经过一番调研，决定以<code>prometheus</code>为核心，搭建监控体系，监控告警的整体架构如下图所示：</p><img src="monitor.png" style="zoom： 50%；" /><p>整个监控模块围绕<code>prometheus</code>进行展开，主要的功能有：</p><ol><li>自研exporter获取线上es集群的关键指标信息，暴露的rest接口能根据请求中的集群名称返回不同集群的监控信息；</li><li>prometheus采取pull模式，定期请求exporter以获取es集群的监控信息；</li><li>grafana配置监控大盘对监控数据进行可视化；</li><li>告警应用给予promQL定期计算是否有指标异常，指标细化到各个es集群；</li><li>告警渠道目前主要采用钉钉，及时发送告警信息到钉钉群，快速定位问题集群、节点信息；</li><li>告警设立优先级，同时触发优先级高的先行发送，直指问题本质；</li><li>延迟告警避免偶发抖动造成的频繁告警与告警恢复；</li><li>诊断模块整合实时信息和历史监控趋势，发现潜在问题，消灭问题于萌芽状态；</li></ol><h4 id="集群监控与告警"><a href="#集群监控与告警" class="headerlink" title="集群监控与告警"></a>集群监控与告警</h4><p>为了保证ES集群的稳定运行，我们需要从多个维度对ES集群进行监控，主要的维度信息如下：</p><ul><li>资源类，包含部署ES机器的cpu、内存、网络、磁盘等信息；</li><li>集群级，ES集群本身是否处于健康状态，从一个较大的维度确认集群是否正常；</li><li>节点级，ES作为一个分布式的应用，每个节点的状态会最终影响集群的状态，需要对节点的jvm、线程池等进行监控；</li></ul><p>最终形成的监控大盘如下：</p><img src="monitor-1.png" style="zoom： 50%；" /><p>告警配置：</p><img src="warn-02.png" style="zoom：50%；" /><p>告警信息：</p><img src="warn-dingding.png"  /><h4 id="集群诊断"><a href="#集群诊断" class="headerlink" title="集群诊断"></a>集群诊断</h4><p>上面的监控告警能帮助我们快速定位集群的现有的问题，但是从长远角度来看，我们需要在问题出现之前提前发现、提前解决，尽可能避免生产故障。带着这样的思考，我们决定提供es集群的诊断能力–将对问题的排查手段和实践经验进行复用，将其标准化，最终沉淀在我们的集群诊断模块中。</p><p>诊断模块的设计思想是核心指标量化，利用量化的指标来帮助我们明确优化方向和快速定位问题，总结日常的问题排查和解决经验，我们将诊断分为五个维度：</p><img src="cluster-judge.png" style="zoom： 50%；" /><p>集群诊断基本涵盖了日常处理es集群问题的大部分场景，通过集群诊断，我们能够提前发现集群的潜在问题，通过提前扩容，更改索引设计与使用方式等行为来规避可能的生产问题。在集群监控和诊断的保驾护航下，2020年至今ESPaaS运维平台管理的ES集群没有发生生产故障。</p><p>诊断建议的界面：</p><img src="zhenduan.png" style="zoom： 50%；" /><h4 id="实践经验"><a href="#实践经验" class="headerlink" title="实践经验"></a>实践经验</h4><p>在监控告警和集群诊断的实际开发与使用过程中，我们也积累了很多的经验。</p><h5 id="1、es集群突然无法写入了，应用日志中全部都是写入失败的记录？"><a href="#1、es集群突然无法写入了，应用日志中全部都是写入失败的记录？" class="headerlink" title="1、es集群突然无法写入了，应用日志中全部都是写入失败的记录？"></a>1、es集群突然无法写入了，应用日志中全部都是写入失败的记录？</h5><p>在实际的问题排查中，发现大部分情况下都是集群部分节点磁盘超过90%触发当前节点的索引read_only，导致无法写入。</p><p>解决措施： </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 解除索引只读</span><br><span class="line">PUT _all/_settings</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;index.blocks.read_only_allow_delete&quot;</span>:<span class="string">&quot;false&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>发生磁盘超过水位线的问题，一方面是对索引或者集群的容量规划做的不到位，另一方面也是监控告警的缺乏导致问题最终发生了。在引入监控告警后，我们对各个ES集群的磁盘设置了告警水位线，80%是告警的阈值，在集群的磁盘超标前，能够发送告警信息，让我们有时间和业务方提前沟通清理数据释放部分磁盘空间。在集群诊断中，我们依据磁盘过去一段时间的使用比例线性预测一天、七天后的使用量，提前和业务方进行沟通，对可能出现问题的集群提前进行扩容，避免出现资源瓶颈。</p><h5 id="2、监控大盘的信息解读"><a href="#2、监控大盘的信息解读" class="headerlink" title="2、监控大盘的信息解读"></a>2、监控大盘的信息解读</h5><p>监控大盘的引入，将监控指标以图表的形式展示出来，让我们能够直观的看到监控指标的当前数值和变化趋势，能帮助我们快速对es集群的资源利用率，性能瓶颈有一个大致的认知.通过下面的监控信息可以看出：</p><ol><li>集群当前是green状态，节点分布是 1master+13data节点；</li><li>主分片数量197，分片数量较少，不会出现单个节点数千个分片的情况；</li><li>多数节点节点在7:00-8:00gc较为频繁，此时段可能为业务高峰期，可能会有大量的写入或者查询等操作；</li><li>整体堆内存占用在50-70%之间，证明集群整体资源暂时没有瓶颈；</li><li>gc次数和gc耗时的面板发现有节点出现毛刺，可以通过分片监控查看是否分片分配不均匀导致了热节点；</li></ol><img src="monitor-2.png" style="zoom：50%；" /><h5 id="3、频繁告警的处理"><a href="#3、频繁告警的处理" class="headerlink" title="3、频繁告警的处理"></a>3、频繁告警的处理</h5><p>在告警功能初次上线时，有些集群的资源使用率比较高，部分节点内存占用超标了，当时的告警策略是一分钟检测一次，如果有异常就产生告警。这会带来频繁告警的问题，如果问题修复的时间较长，那每分钟都会产生一次告警信息，造成告警信息轰炸，告警群的告警信息全部都是同一条，不禁让人厌烦，还会导致开发小伙伴忽略其他的告警内容。</p><p>面对这种情况，参考一些业界的告警设计，在初次告警后，再次触发的告警不再立马发出，而是给予不同的时间间隔，如果在告警半小时后，问题没有解决，指标依旧异常，则产生第二次钉钉告警，告警间隔逐级顺延直至恢复，大大减少了重复的告警数量。</p><h5 id="4、延迟告警的设计"><a href="#4、延迟告警的设计" class="headerlink" title="4、延迟告警的设计"></a>4、延迟告警的设计</h5><p>内部的日志es集群，会存储近2月的日志索引，但是7天前的默认会关闭，由于经常有同学需要排查历史问题需要开启已经closed掉的索引，在索引开启的过程中，会造成日志es集群的短暂red/yellow状态，导致触发告警后又立即恢复，在钉钉告警群中大量刷消息。</p><p>面对这种能够自恢复的问题，我们决定设置延迟告警的策略，如果在告警触发的数分钟内(可配置)，告警指标恢复正常则不再进行告警，将这类偶发的抖动问题变成静默状态，不再干扰正常的告警。</p><h4 id="脚踏实地，展望未来"><a href="#脚踏实地，展望未来" class="headerlink" title="脚踏实地，展望未来"></a>脚踏实地，展望未来</h4><ol><li><p>ES容器化</p><blockquote><p>kubernetes的应用越来越广泛，ES+k8s将是中通在有状态服务上的探索与尝试；</p></blockquote></li><li><p>ES-Proxy–提供搜索服务</p><blockquote><p>未来希望将ES提供的搜索能力标准化，用户通过proxy使用ES，不必关心具体ES集群部署和索引分布；</p></blockquote></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h4 id=&quot;背景介绍：&quot;&gt;&lt;a href=&quot;#背景介绍：&quot; class=&quot;headerlink&quot; title=&quot;背景介绍：&quot;&gt;&lt;/a&gt;背景介绍：&lt;/h4&gt;&lt;p&gt;中通在2015年的时候已经开始预研并在生产环境使用Elasticsearch集群，随后在科技中心开始大规模实践。随着</summary>
      
    
    
    
    
    <category term="ES" scheme="https://zto-express.github.io/tags/ES/"/>
    
    <category term="搜索" scheme="https://zto-express.github.io/tags/%E6%90%9C%E7%B4%A2/"/>
    
    <category term="运维" scheme="https://zto-express.github.io/tags/%E8%BF%90%E7%BB%B4/"/>
    
    <category term="平台" scheme="https://zto-express.github.io/tags/%E5%B9%B3%E5%8F%B0/"/>
    
    <category term="集群" scheme="https://zto-express.github.io/tags/%E9%9B%86%E7%BE%A4/"/>
    
    <category term="监控" scheme="https://zto-express.github.io/tags/%E7%9B%91%E6%8E%A7/"/>
    
  </entry>
  
  <entry>
    <title>Kafka 节点重启失败导致数据丢失的分析排查与解决之道</title>
    <link href="https://zto-express.github.io/2020/08/15/kafka/"/>
    <id>https://zto-express.github.io/2020/08/15/kafka/</id>
    <published>2020-08-15T02:32:54.000Z</published>
    <updated>2020-08-15T09:12:42.099Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在 2 月10 号下午大概 1 点半左右，收到用户方反馈，发现日志 kafka 集群 A 主题 的 34 分区选举不了 leader，导致某些消息发送到该分区时，会报如下 no leader 的错误信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In the middle of a leadership election, there is currently no leader for this partition and hence it is unavailable for writes.</span><br></pre></td></tr></table></figure><p>由于 A 主题 34 分区的 leader 副本在 broker0，另外一个副本由于速度跟不上 leader，已被踢出 ISR，0.11 版本的 kafka 的 unclean.leader.election.enable 参数默认为 false，表示分区不可在 ISR 以外的副本选举 leader，导致了 A 主题发送消息持续报 34 分区 leader 不存在的错误，且该分区还未消费的消息不能继续消费了。</p><p>接下来运维在 kafka-manager 查不到 broker0 节点了处于假死状态，但是进程依然还在，重启了好久没见反应，然后通过 kill -9 命令杀死节点进程后，接着重启失败了，导致了如下问题。</p><p>Kafka 日志分析</p><p>查看了 KafkaServer.log 日志，发现 Kafka 重启过程中，产生了大量如下日志：</p><p><img src="__640.jpeg"></p><p>发现大量主题索引文件损坏并且重建索引文件的警告信息，定位到源码处：</p><p>kafka.log.OffsetIndex#sanityCheck<br><img src="___640.jpeg"></p><p><strong>按我自己的理解描述下：</strong></p><p>Kafka 在启动的时候，会检查 kafka 是否为 cleanshutdown，判断依据为 ${log.dirs} 目录中是否存在 .kafka_cleanshutDown 的文件，如果非正常退出就没有这个文件，接着就需要 recover log 处理，在处理中会调用 sanityCheck() 方法用于检验每个 log sement 的 index 文件，确保索引文件的完整性：</p><ul><li><p>entries：由于 kafka 的索引文件是一个稀疏索引，并不会将每条消息的位置都保存到 .index 文件中，因此引入了 entry 模式，</p></li><li><p>即每一批消息只记录一个位置，因此索引文件的 entries = mmap.position / entrySize；</p></li><li><p>lastOffset：最后一块 entry 的位移，即 lastOffset = lastEntry.offset；</p></li><li><p>baseOffset：指的是索引文件的基偏移量，即索引文件名称的那个数字。</p></li></ul><p><strong>索引文件与日志文件对应关系图如下：</strong><br><img src="____640.jpeg"></p><p><strong>判断索引文件是否损坏的依据是：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">_entries &#x3D;&#x3D; 0 || _lastOffset &gt; baseOffset &#x3D; false &#x2F;&#x2F; 损坏</span><br><span class="line"></span><br><span class="line">_entries &#x3D;&#x3D; 0 || _lastOffset &gt; baseOffset &#x3D; true &#x2F;&#x2F; 正常</span><br></pre></td></tr></table></figure><p><strong>这个判断逻辑我的理解是：</strong></p><p>entries 索引块等于零时，意味着索引没有内容，此时可以认为索引文件是没有损坏的；当 entries 索引块不等于 0，就需要判断索引文件最后偏移量是否大于索引文件的基偏移量，如果不大于，则说明索引文件被损坏了，需要用重新构建。</p><p>那为什么会出现这种情况呢？</p><p>我在相关 issue 中似乎找到了一些答案：</p><p><img src="_____640.jpeg"><br><a href="https://issues.apache.org/jira/browse/KAFKA-1112">https://issues.apache.org/jira/browse/KAFKA-1112</a></p><p><a href="https://issues.apache.org/jira/browse/KAFKA-1554">https://issues.apache.org/jira/browse/KAFKA-1554</a></p><p>总的来说，非正常退出在旧版本似乎会可能发生这个问题？</p><p>有意思的来了，导致开机不了并不是这个问题导致的，因为这个问题已经在后续版本修复了，从日志可看出，它会将损坏的日志文件删除并重建， <strong>我们接下来继续看导致重启不了的错误信息：</strong></p><p><img src="______640.jpeg"></p><p>问题就出在这里，在删除并重建索引过程中，就可能出现如上问题，在 issues.apache.org 网站上有很多关于这个 bug 的描述，我这里贴两个出来：</p><p><a href="https://issues.apache.org/jira/browse/KAFKA-4972">https://issues.apache.org/jira/browse/KAFKA-4972</a></p><p><a href="https://issues.apache.org/jira/browse/KAFKA-3955">https://issues.apache.org/jira/browse/KAFKA-3955</a></p><p>这些 bug 很隐晦，而且非常难复现，既然后续版本不存在该问题，当务之急还是升级 Kafka 版本，后续等我熟悉 scala 后，再继续研究下源码，细节一定是会在源码中呈现。</p><p>解决思路分析</p><p>针对背景两个问题，矛盾点都是因为 broker0 重启失败导致的，那么我们要么把 broker0 启动成功，才能恢复 A 主题 34 分区。</p><p>由于日志和索引文件的原因一直启动不起来，我们只需要将损坏的日志和索引文件删除并重启即可。但如果出现 34 分区的日志索引文件也损坏的情况下，就会丢失该分区下未消费的数据，原因如下：</p><p>此时 34 分区的 leader 还处在 broker0 中，由于 broker0 挂掉了且 34 分区 isr 只有 leader，导致 34 分区不可用，在这种情况下，假设你将 broker0 中 leader 的数据清空，重启后 Kafka 依然会将 broker0 上的副本作为 leader，那么就需要以 leader 的偏移量为准，而这时 leader 的数据清空了，只能将 follower 的数据强行截断为 0，且不大于 leader 的偏移量。</p><p>这似乎不太合理，这时候是不是可以提供一个操作的可能：</p><p>在分区不可用时，用户可以手动设置分区内任意一个副本作为 leader？</p><p>下面我会对这个问题进行分析。</p><p>后续集群的优化</p><p>1、制定一个升级方案，将集群升级到 2.x 版本；</p><p>2、每个节点的服务器将 systemd 的默认超时值为 600 秒，因为我发现运维在故障当天关闭 33 节点时长时间没反应，才会使用 kill -9 命令强制关闭。</p><p>但据我了解关闭一个 Kafka 服务器时，Kafka 需要做很多相关工作，这个过程可能会存在相当一段时间，而 systemd 的默认超时值为 90 秒即可让进程停止，那相当于非正常退出了。</p><p>3、将 broker 参数 unclean.leader.election.enable 设置为 true（确保分区可从非 ISR 中选举 leader）；</p><p>4、将 broker 参数 default.replication.factor 设置为 3（提高高可用，但会增大集群的存储压力，可后续讨论）；</p><p>5、将 broker 参数 min.insync.replicas 设置为 2（这么做可确保 ISR 同时有两个，</p><p>但是这么做会造成性能损失，是否有必要？因为我们已经将 unclean.leader.election.enable 设置为 true 了）；</p><p>6、发送端发送 acks=1（确保发送时有一个副本是同步成功的，但这个是否有必要，因为可能会造成性能损失）。</p><p>从源码中定位到问题的根源</p><p>首先把导致 Kafka 进程退出的异常栈贴出来：<br><img src="_______640.jpeg"></p><p><em>注：以下源码基于 kafka 0.11.x 版本。</em></p><p>我们直接从 index 文件损坏警告日志的位置开始：</p><p>kafka.log.Log#loadSegmentFiles<br><img src="________640.jpeg"><br>从前一篇文章中已经说到，Kafka 在启动的时候，会检查kafka是否为 cleanshutdown，判断依据为 ${log.dirs} 目录中是否存在 .kafka_cleanshutDown 的文件，如果非正常退出就没有这个文件，接着就需要 recover log 处理，在处理中会调用 。</p><p>在 recover 前，会调用 sanityCheck() 方法用于检验每个 log sement 的 index 文件，确保索引文件的完整性 ，如果发现索引文件损坏，删除并调用 recoverSegment() 方法进行索引文件的重构，最终会调用 recover() 方法：</p><p>kafka.log.LogSegment#recover<br><img src="_________640.jpeg"><br><strong>源码中相关变量说明：</strong></p><ul><li><p>log：当前日志 Segment 文件的对象；</p></li><li><p>batchs：一个 log segment 的消息压缩批次；</p></li><li><p>batch：消息压缩批次；</p></li><li><p>indexIntervalBytes：该参数决定了索引文件稀疏间隔打底有多大，由 broker 端参数 log.index.interval.bytes 决定，默认值为 4 KB，即表示当前分区 log 文件写入了 4 KB 数据后才会在索引文件中增加一个索引项（entry）；</p></li><li><p>validBytes：当前消息批次在 log 文件中的物理地址。</p></li></ul><p>知道相关参数的含义之后，那么这段代码的也就容易解读了：循环读取 log 文件中的消息批次，并读取消息批次中的 baseOffset 以及在 log 文件中物理地址，将其追加到索引文件中，追加的间隔为 indexIntervalBytes 大小。</p><p><strong>我们再来解读下消息批次中的 baseOffset：</strong></p><p>我们知道一批消息中，有最开头的消息和末尾消息，所以一个消息批次中，分别有 baseOffset 和 lastOffset，源码注释如下：<br><img src="__________640.jpeg"><br>其中最关键的描述是：它可以是也可以不是第一条记录的偏移量。kafka.log.OffsetIndex#append<br><img src="___________640.jpeg"><br>以上是追加索引块核心方法，在这里可以看到 Kafka 异常栈的详细信息，Kafka 进程也就是在这里被异常中断退出的。（这里吐槽一下，为什么一个分区有损坏，要整个 broker 挂掉？宁错过，不放过？就不能标记该分区不能用，然后让 broker 正常启动以提供服务给其他分区吗？建议 Kafka 在日志恢复期间加强异常处理，不知道后续版本有没有优化，后面等我拿 2.x 版本源码分析一波）</p><p><strong>退出的条件是：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">_entries &#x3D;&#x3D; 0 || offset &gt; _lastOffset &#x3D; false</span><br></pre></td></tr></table></figure><p>也就是说，假设索引文件中的索引条目为 0，说明索引文件内容为空，那么直接可以追加索引，而如果索引文件中有索引条目了，需要消息批次中的 baseOffset 大于索引文件最后一个条目中的位移，因为索引文件是递增的，因此不允许比最后一个条目的索引还小的消息位移。</p><p>现在也就很好理解了，产生这个异常报错的根本原因，是因为后面的消息批次中，有位移比最后索引位移还要小（或者等于）。</p><p>前面也说过了，消息批次中的 baseOffset 不一定是第一条记录的偏移量，那么问题是不是出在这里？我的理解是这里有可能会造成两个消息批次获取到的 baseOffset 有相交的值？</p><p>对此我并没有继续研究下去了，但我确定的是，在 kafka 2.x版本中，append() 方法中的 offset 已经改成 消息批次中的 lastOffset 了：<br><img src="____________640.jpeg"><br>这里我也需要吐槽一下，如果出现这个 bug，意味着这个问题除非是将这些故障的日志文件和索引文件删除，否则该节点永远启动不了，这也太暴力了吧？</p><p>我花了非常多时间去专门看了很多相关 issue，目前还没看到有解决这个问题的方案？</p><p>或者我需要继续寻找？我把相关 issue 贴出来：</p><p><a href="https://issues.apache.org/jira/browse/KAFKA-1211">https://issues.apache.org/jira/browse/KAFKA-1211</a></p><p><a href="https://issues.apache.org/jira/browse/KAFKA-3919">https://issues.apache.org/jira/browse/KAFKA-3919</a></p><p><a href="https://issues.apache.org/jira/browse/KAFKA-3955">https://issues.apache.org/jira/browse/KAFKA-3955</a></p><p><strong>严重建议各位尽快把 Kafka 版本升级到 2.x 版本，旧版本太多问题了，后面我着重研究 2.x 版本的源码。</strong></p><p><strong>下面我从日志文件结构中继续分析。</strong></p><p>从日志文件结构中看到问题的本质</p><p>我们用 Kafka 提供的 DumpLogSegments 工具打开 log 和 index 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ~&#x2F;kafka_2.1x-0.11.x&#x2F;bin&#x2F;kafka-run-class.sh kafka.tools.DumpLogSegments --files &#123;log_path&#125;&#x2F;secxxx-2&#x2F;00000000000110325000.log &gt; secxxx.log$ ~&#x2F;kafka_2.1x-0.11.x&#x2F;bin&#x2F;kafka-run-class.sh kafka.tools.DumpLogSegments --files &#123;log_path&#125;&#x2F;secxxx-2&#x2F;00000000000110325000.index &gt; secxxx-index.log</span><br></pre></td></tr></table></figure><p>用 less -Nm 命令查看，log 和 index 对比：</p><p><img src="_____________640.jpeg"><br>如上图所示，index最后记录的 offset = 110756715，positioin=182484660，与异常栈显示的一样，说明在进行追加下一个索引块的时候，发现下一个索引块的 offset 索引不大于最后一个索引块的 offset，因此不允许追加，报异常并退出进程，那么问题就出现在下一个消息批次的 baseOffset，根据 log.index.interval.bytes 默认值大小为 4 KB（4096），而追加的条件前面也说了，需要大于 log.index.interval.bytes，因此我们 DumpLogSegments 工具查询：</p><p><img src="______________640.jpeg"><br>从 dump 信息中可知，在 positioin=182484660 往后的几个消息批次中，它们的大小加起来大于 4096 的消息批次的 offset=110756804，postion=182488996，它的 baseOffset 很可能就是 110756715，与索引文件最后一个索引块的 Offset 相同，因此出现错误。</p><p>接着我们继续用 DumpLogSegments 工具查看消息批次内容：</p><p>我们先查看 offset = 110756715，positioin=182484660 的消息块详情：<br><img src="_______________640.jpeg"><br>接着寻找 offset = 110756715，的消息批次块：</p><p><img src="________________640.jpeg"><br>终于找到你了，跟我预测的一样！postion=182488996，在将该消息批次追加到索引文件中，发生 offset 混乱了。</p><p>如果还是没找到官方的处理方案，就只能删除这些错误日志文件和索引文件，然后重启节点？</p><p>非常遗憾，我在查看了相关的 issue 之后，貌似还没看到官方的解决办法，所幸的是该集群是日志集群，数据丢失也没有太大问题。</p><p>我也尝试发送邮件给 Kafka 维护者，期待大佬的回应：</p><p><img src="_________________640.jpeg"></p><p>不过呢，0.11.x 版本属于很旧的版本了，因此，升级 Kafka 版本才是长久之计啊！我已经迫不及待地想撸 kafka 源码了！</p><p>经过以上问题分析与排查之后，我专门对分区不可用进行故障重现，并给出我的一些骚操作来尽量减少数据的丢失。</p><p>故障重现</p><p>下面我用一个例子重现现分区不可用且 leader 副本被损坏的例子：</p><ol><li><p>使用 unclean.leader.election.enable = false 参数启动 broker0；</p></li><li><p>使用 unclean.leader.election.enable = false 参数启动 broker1；</p></li><li><p>创建 topic-1，partition=1，replica-factor=2；</p></li><li><p>将消息写入 topic-1；</p></li><li><p>此时，两个 broker 上的副本都处于 ISR 中，broker0 的副本为 leader 副本；</p></li><li><p>停止 broker1，此时 topic-1 的 leader 依然是 broker0 的副本，而 broker1 的副本从 ISR 中剔除；</p></li><li><p>停止 broker0，并且删除 broker0 上的日志数据；</p></li><li><p>重启 broker1，topic-1 尝试连接 leader 副本，但此时 broker0 已经停止运行，此时分区处于不可用状态，无法写入消息；</p></li><li><p>恢复 broker0，broker0 上的副本恢复 leader 职位，此时 broker1 尝试加入 ISR，但此时由于 leader 的数据被清除，即偏移量为 0，此时 broker1 的副本需要截断日志，保持偏移量不大于 leader 副本，此时分区的数据全部丢失。</p></li></ol><p>向Kafka官方提的建议</p><p>在遇到分区不可用时，是否可以提供一个选项，让用户可以手动设置分区内任意一个副本作为 leader？</p><p>因为集群一旦设置了 unclean.leader.election.enable = false，就无法选举 ISR 以外的副本作为 leader，在极端情况下仅剩 leader 副本还在 ISR 中，此时 leader 所在的 broker 宕机了。</p><p>那如果此时 broker 数据发生损坏这么办？在这种情况下，能不能让用户自己选择 leader 副本呢？尽管这么做也是会有数据丢失，但相比整个分区的数据都丢失而言，情况还是会好很多的。</p><p>如何尽量减少数据丢失</p><p>首先你得有一个不可用的分区（并且该分区 leader 副本数据已损失），如果是测试，可以以上故障重现 1-8 步骤实现一个不可用的分区（需要增加一个 broker）：<br><img src="__________________640.jpeg"><br>此时 leader 副本在 broker0，但已经挂了，且分区不可用，此时 broker2 的副本由于掉出 ISR ，不可选为 leader，且 leader 副本已损坏清除，如果此时重启 broker0，follower 副本会进行日志截断，将会丢失该分区所有数据。</p><p>经过一系列的测试与实验，我总结出了以下骚操作，可以强行把 broker2 的副本选为 leader，尽量减少数据丢失：</p><p>1、使用 kafka-reassign-partitions.sh 脚本对该主题进行分区重分配，当然你也可以使用 kafka-manager 控制台对该主题进行分区重分配，重分配之后如下：</p><p><img src="___________________640.jpeg"><br>此时 preferred leader 已经改成 broker2 所在的副本了，但此时的 leader 依然还是 broker0 的副本。需要注意的是，分区重分配之后的 preferred leader 一定要之前那个踢出 ISR 的副本，而不是分区重分配新生成的副本。因为新生成的副本偏移量为 0，如果自动重分配不满足，那么需要编写 json 文件，手动更改分配策略。</p><p>2、进入 zk，查看分区状态并修改它的内容：<br><img src="____________________640.jpeg"><br>修改 node 内容，强行将 leader 改成 2（与重分配之后的 preferred leader 一样），并且将 leader_epoch 加 1 处理，同时 ISR 列表改成 leader，改完如下：<br><img src="_____________________640.jpeg"><br>此时，kafka-manager 控制台会显示成这样：<br><img src="______________________640.jpeg"><br>但此时依然不生效，记住这时需要重启 broker 0。</p><p>3、重启 broker0，发现分区的 lastOffset 已经变成了 broker2 的副本的 lastOffset：<br><img src="_______________________640.jpeg"><br>成功挽回了 46502 条消息数据，尽管依然丢失了 76053 - 46502 = 29551 条消息数据，但相比全部丢失相对好吧！</p><p>以上方法的原理其实很简单，就是强行把 Kafka 认定的 leader 副本改成自己想要设置的副本，然后 lastOffset 就会以我们手动设置的副本 lastOffset 为基准了。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;在 2 月10 号下午大概 1 点半左右，收到用户方反馈，发现日志 kafka 集群 A 主题 的 34 分区选举不了 leader，导致某</summary>
      
    
    
    
    
    <category term="集群" scheme="https://zto-express.github.io/tags/%E9%9B%86%E7%BE%A4/"/>
    
    <category term="MQ" scheme="https://zto-express.github.io/tags/MQ/"/>
    
    <category term="Kafka" scheme="https://zto-express.github.io/tags/Kafka/"/>
    
    <category term="消息队列" scheme="https://zto-express.github.io/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"/>
    
    <category term="日志" scheme="https://zto-express.github.io/tags/%E6%97%A5%E5%BF%97/"/>
    
  </entry>
  
  <entry>
    <title>中通消息服务运维平台实践（已开源）</title>
    <link href="https://zto-express.github.io/2020/08/15/zms/"/>
    <id>https://zto-express.github.io/2020/08/15/zms/</id>
    <published>2020-08-14T16:28:50.000Z</published>
    <updated>2020-08-15T03:36:53.906Z</updated>
    
    <content type="html"><![CDATA[<p>中通快递每天有数千万的运单在各个环节运转，每个环节都有对应的多套业务系统来支撑，业务系统之间上下游关系较为密切，从上游的客户订单到下游转运、结算、分析等每个环节都离不开消息中间件，它主要解决了系统之前的耦合、业务的削峰填谷、异步通信、数据同步和冗余存储等等功能需求，是现有系统架构中，不同系统之间交互的主要方式之一。</p><p>在2015年中通开始大量采用消息中间解决一些特定的问题，随着业务的增长，各环节有了更精细化的产品，我们消息中间件的数据体量越来越大，集群规模越来越多，中间件也越来越多样化，统一管理这些消息中间件变得尤为重要，因此我们研发了中通消息中间件平台ZMS，主要基于RocketMQ+Kafka两套业界比较主流消息中间件，提供了自动化部署、主题/消费者的申请审核、统一的SDK、管理控制台、监控告警到无感知扩容迁移等一系列运维的功能，目前ZMS管理了17个集群，包括7个Kafka集群和10个RocketMQ集群，主题1000多个，消费组3000多个，日均消息流转达到百亿级。</p><p>ZMS从最初的版本演进到现在，基本围绕我们的最初设计的功能，根据每个阶段的痛点不同，解决不同的问题。整个过程，根据不同阶段的不同输出，大体可以三个维度。<br><img src="1.png" alt="1"></p><p>经公司内部评估，ZMS 已经成长为一套相对成熟的消息中间件云平台化解决方案，可以正式对外开放，与社会上的同行共同打磨，故决定于2020年5月26号正式开源，将代码推送到github 仓库，开源地址信息如下：</p><p>官方开源地址：<a href="https://gitee.com/zto_express/zms">https://gitee.com/zto_express/zms</a></p><h2 id="1、自动化运维与部署"><a href="#1、自动化运维与部署" class="headerlink" title="1、自动化运维与部署"></a>1、自动化运维与部署</h2><p>自动化运维部署，主要是方便运维人员可以快速通过ZMS平台向导式初始化一个集群，其架构设计如下图所示。</p><p><img src="2.png" alt="2"></p><p>zms-portal：zms 管理后台，可同时管理多个环境的资源，包括：添加主机、服务，消息集群状态监控、配置消息集群告警规则，消息集群资源管理等。</p><blockquote><p> 备注：所谓的环境我们可以简单看出开发环境、测试环境、高保真环境、生产环境。</p></blockquote><p>在每台机器上首先需要安装 zms-agent (代理服务)、supervisor 等基础组件，为了方便运维，ZMS 提供了一键初始化主机的脚本，其操作流程如下：</p><p><img src="3.png" alt="3"></p><p>运维人员只需要去指定的机器上复制上述命令即可。这样实现的目的是 zms-portal 无需管理宿主机器的用户名、密码等敏感信息，做到安全可控。zms 能自动感知安装了 zms-agent 的机器并将其纳入 zms-portal 的管理运维体系。</p><p>在 ZMS 中我们统一将 Kafka、RocketMQ、ZK、指标收集、监控告警等统一看成是一个服务，在不同的环境中可以选择性的安装，其操作如下图所示：</p><p><img src="4.png" alt="4"></p><h2 id="2、统一的客户端-SDK"><a href="#2、统一的客户端-SDK" class="headerlink" title="2、统一的客户端 SDK"></a>2、统一的客户端 SDK</h2><p>基于中通业务的特点在具体项目中采用了Kafka与RocketMQ 两种不同的消息中间件，如果业务方在自己项目中既要使用 Kafka 的消息中间件，又要使用 RocketMQ 的消息中间件，对于消息中间件的使用来说要求非常高，因为需要了解这些相似又不同的 API，分别了解其配置参数与其代表的含义，因此为业务方提供统一的API显得尤为重要与迫切。</p><p>zms-sdk 的主要设计理念：</p><ul><li>屏蔽底层消息中间件类型，提供统一标准的API。</li><li>提供标准的埋点，方便打造完备的监控体系。</li><li>云平台化，开发人员只需关注TOPIC、消费组本身，无需关注 TOPIC 是存储在哪个集群上，即将 TOPIC、消费组资源化，用户只需按需向平台申请 topic、消费组即可。</li></ul><p>zms-sdk 的整体架构设计如图所示：</p><p><img src="5.png" alt="5"></p><p>主要的交互与设计要点如下：</p><ul><li>运维人员通过 zms-portal 在线安装集群，集群的元数据将会存在 zookeeper 中。</li><li>开发人员通过在 zms-portal 申请 topic、消费组。</li><li>运维人员在 zms-portal 中对用户的申请进行审批，并根据用户的需求分配到适合的集群中，topic 所在集群等元信息会写入到 zookeeper中。</li><li>开发人员通过 zms-sdk 向所申请的 topic 发送消息，zms-sdk 内部会从 zk 获取 topic 对应的元信息并创建对应的客户端，最终完成消息的发送。</li></ul><p>整个设计的核心是引入 zookeeper 作为元数据的存储仓库，并充分利用其事件监听机制，能完成很多“高大上”的功能，例如主题在线迁移功能。</p><p>试想一下在双十一等大促场景，如果一个集群负载很高从而达到瓶颈，一方面是可以对集群进行扩容，另外一种可行的方法时将该集群中的 topic 迁移到其他空间集群，正是依托于 zookeeper 的事件机制，应用客户端无需重启就可以自动感知 topic 的配置发生了变化，从而重新构建到新集群的客户端对象，完成消息发送的不停机在线迁移。</p><h2 id="3、监控数据采集服务"><a href="#3、监控数据采集服务" class="headerlink" title="3、监控数据采集服务"></a>3、监控数据采集服务</h2><p>对消息中间件进行监控并进行可视化展示是运维最基本的需求，RocketMQ、Kafka 消息中间件本身提供了监控数据的采集并存储在各自的服务端内存，并且是非持久化的，在内存中只存储当前时间段的调用信息，并随着时间的推进，旧的数据将被删除。当然 RocketMQ、Kafka 都提供了相应的API方便客户端采集存储在服务端内存中的监控数据。</p><p>ZMSCollector 的职责就是定时向 RocketMQ、Kafka 集群采集相关的调用信息并持久化到 influxdb中，为后续的可视化展示提供必要的基础，ZMSCollector 已经被服务化，可以通过 zms-portal 在线安装。</p><p>ZMSCollector 的整体架构设计如下图所示：</p><p><img src="6.png" alt="6"></p><p>ZMSCollector 的整体设计比较简单，一方面通过定时调度的方式调用底层消息中间件提供的API，将监控指标存储到 influxDb，另外一方面采集 zms-sdk 采集的监控数据，zms-sdk采集的监控数据会发送的一个固定的 topic ，ZMSCollector 订阅指定的 topic，对消息进行加工后存储在 influxDb中。</p><h2 id="4、多机房解决方案"><a href="#4、多机房解决方案" class="headerlink" title="4、多机房解决方案"></a>4、多机房解决方案</h2><p>目前中通在异地容灾方面还刚刚起步，目前只需要实现同城机房冷备，即一个机房的入口网络发生故障，需要将流量切换到另外一个备份机房。ZMS 消息中间件运维平台天然支持多机房的部署架构，因为在 ZMS 的“眼中”，一个不同的机房就相当于一个环境，可以直接在 zms-portal 中完成一个新机房的安装部署服务。但由于发生故障后，两个机房内部的网络有可能会断开，故两个机房中的元数据应该分开存储，即 zms-sdk 所依懒的 zookeeper 集群不同，故需要完成 zookeeper 元数据的同步，该工作由 ZMSBackupCluster 服务来承担，其架构设计如下图所示：</p><p><img src="7.png" alt="7"></p><p>其基本的设计思路是 ZMSBackupCluster 订阅待同步机房的 zookeeper，一旦元数据有发生变化，会按照配置集群元数据映射关系将其同步到目标机房的 zookeeper中，这样部署在备份机房中的应用可以无感知的接管主机房中所有的消息发送与消息消费任务。</p><h2 id="5、zms-portal-部分界面展示"><a href="#5、zms-portal-部分界面展示" class="headerlink" title="5、zms-portal 部分界面展示"></a>5、zms-portal 部分界面展示</h2><p>zms-portal 提供了集群自动化安装部署、主题消费组审批、各种监控报表可视化报表，接下来展示部分界面，详细请移步 zms 开源仓库。</p><p><img src="8.png" alt="8"></p><p><img src="9.png" alt="9"></p><p><img src="10.png" alt="10"></p><p><img src="11.png" alt="11"></p><p><img src="12.png" alt="12"></p><p><img src="13.png" alt="13"></p><p><img src="14.png" alt="14"></p><h2 id="6、ZMS-开源信息"><a href="#6、ZMS-开源信息" class="headerlink" title="6、ZMS 开源信息"></a>6、ZMS 开源信息</h2><p>中通科技正式开源内部的消息Pass云平台化产品ZMS，其开源仓库地址： github ，包含了 ZMS 的使用说明、架构设计文档、技术交流群。开源只是完成万里长征第一步，后续希望更多的开源爱好者加入到该项目中，共同打造一体化的智能消息运维平台。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;中通快递每天有数千万的运单在各个环节运转，每个环节都有对应的多套业务系统来支撑，业务系统之间上下游关系较为密切，从上游的客户订单到下游转运、结算、分析等每个环节都离不开消息中间件，它主要解决了系统之前的耦合、业务的削峰填谷、异步通信、数据同步和冗余存储等等功能需求，是现有系</summary>
      
    
    
    
    
    <category term="中通" scheme="https://zto-express.github.io/tags/%E4%B8%AD%E9%80%9A/"/>
    
    <category term="消息" scheme="https://zto-express.github.io/tags/%E6%B6%88%E6%81%AF/"/>
    
    <category term="MQ" scheme="https://zto-express.github.io/tags/MQ/"/>
    
    <category term="RocketMQ" scheme="https://zto-express.github.io/tags/RocketMQ/"/>
    
    <category term="Kafka" scheme="https://zto-express.github.io/tags/Kafka/"/>
    
  </entry>
  
</feed>
